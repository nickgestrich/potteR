---
title: "Example workflow for ceramic analysis using potteR"
author: "Nick Gestrich"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ceramic_analysis_workflow}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
## REMOVE after dev
library(devtools)
load_all()
```

```{r setup}
library(potteR)
library(dplyr)
library(tidyr)
library(ggplot2)
```

## Example Data

This example runs on data from a 2023 excavation in the Republic of
Guinea. These are recorded using a modified version of the system
proposed by Jesse & Nowotnick (2022). It is included in the package.

```{r}
rims <- guinea[[1]]
body <- guinea[[2]]

#some of the observations on rims are not made on body sherds. We will treat these observations separately
all <- guinea[[3]]

r <- names(rims)[! names(rims) %in% names(body)]

rims_only <- rims |> 
  select(all_of(r), site, sondage, context, unit_id, sond_con)
```

## 1. Univariate summary statistics

We begin our explorative data analysis with a report of summary
statistics and counts of each variable. This means counts of unique
values in the categorical variables, and summaries of the numeric
variables. PotteR provides these as an R object in a list, and as a pdf
report for perusal outside of R. The pdf version also includes a basic
visualisation for each variable.

```{r}
# make a list of summaries for every variable
rim_summary <- get_summary(rims_only)

# to generate a pdf report, uncomment and run the code below

# print_summary(rims_only, file = here::here("summary.pdf"))



# NB possibly replace with the dfSummary() command from here:
# https://htmlpreview.github.io/?https://github.com/dcomtois/summarytools/blob/master/doc/introduction.html#grouped-statistics-group_by

library(summarytools)
view(dfSummary(rims))
# This would have to be wrapped in a markdown function to give out a nice report or print to pdf.
```

It will often not be necessary to run these functions on the entire
dataset. In that case, a selection of variables can be made first.

```{r}
#| output: false
rims_only |> 
  select(parts:rim_type) |> 
  get_summary()
```

## 2. Fixing errors in the data

The univariate reports frequently show up errors in the data. These
should be fixed and the reports rerun before moving on to further
analyses.

### Replace individual values

The package provides a small wrapper function to replace individual
cases.

```{r}
# Put this function elsewhere
replace_values <- function(data, col, old, new){
  data |> 
    dplyr::mutate({{col}} := replace({{col}}, {{col}} == old, new))
}

rims_only <- rims_only |> replace_values(rim_angle, 43, 4)
```

Note that this method does not apply to NA values.

### Setting and replacing NA values

To replace all NAs in a variable with a different value (e.g. 0), use
the replace_na() function from the tidyr package.

```{r}
rims_only$rim_angle |> replace_na("missing")
```

To replace a value with NA, use the na_if() function from the dplyr
package. Also see the documentation
[here](https://dplyr.tidyverse.org/reference/na_if.html).

```{r}
rims_only |> mutate(diam = na_if(diam, 0))

# do this for several columns using across()
rims_only |> 
  mutate(across(where(is.numeric), ~na_if(., 0)))
```

### Making several replacements

More numerous replacements are best done using dplyr's case_match()
function in the following manner:

```{r}
rims_only <- rims_only |> 
  dplyr::mutate(rim_type = case_match(rim_type, 
                                      c("SI", "Si") ~ "S1",
                                      c("T1(?)", "T1...") ~ "T1",
                                      .default = rim_type
                                      ))
```

After all the data tidying is done, and has been checked, move on to the
next step. Don't cut corners here, you won't thank yourself later.

## 3. Splitting the data

After cleaning and initial univariate checks on your data, this is
probably a good stage to: \* make some thematic splits (e.g. between
variables that capture the shape, the decor, and modifications). You can
combine this again later. \* discard variables that suffer from having
few or uneven observations and can therefore not be used for statistical
work

### Split the data into topics

How you separate the data is down to your own research questions and
will be informed by your expertise as a ceramic analyst.

From the example data, we will make one dataset that can tell us about
the shape of the vessels and one that can tell us about d√©cor and other
surface treatment. Finally, we will make one that can tell us about
technical aspects of the pottery.

For the shape data, we will only need variables observed on the rim
sherds. We will select the opening diameter, the maximum and minimum
thickness measurements, the rim angle, and the rim type, along with the
variables that inform us about the id and the context of the find.

```{r data splitting 1}
shape_data <- rims_only |> select(diam, mx_th, min_th, rim_angle, rim_type, base, parts, site, sondage, context, unit_id, sond_con)
```

For the decor data, we can make two tables. One in which all surface
treatment data from all sherds is placed together, and one only from rim
sherds, where the decorative motifs are recorded with their location on
the pot.

```{r data splitting 2}
# All decor
all_decor_data <- all |> select(site, sondage, context, sond_con, unit_id, n_sh, surf, dec1:mot3)

# Decor with location
decor_loc_data <- rims |> select(site, sondage, context, sond_con, unit_id, dec1:l3)
```

The technical data is encoded in just a few columns

```{r}
technical_data <- all |> select(site, sondage, context, sond_con, unit_id, n_sh, col, temp, inc, fir)
```

We now have four different datasets. The following steps will be carried
out one by one on each dataset. When we have successfully identified
groups, we can put these different aspects of the ceramics back together
to see how they relate to one another. For the sake of this
demonstration, we will mainly work on the `shape_data` dataset.

### Discard variables

Not all variables have observations that are sufficient in number and
quality to include in further analysis. You can make decisions on which
variables to keep based on the univariate report, or external factors.
You can also have some quantitative help by counting the number of
missing values in the different columns.

```{r missing values}
count_missing(shape_data)
```

In this case, the variable `base` has not been observed in 96% of cases.
We should exclude this, as quantitative analysis will not be possible on
this. However, we should make sure to individually describe the three
bases in the assemblage. It is useful to do this straight away, as the
variable can then be excluded with a clear conscience.

```{r exclude variables}
shape_data <- shape_data |> select(-base)
```

We now have a good dataset with which to continue into statistical
exploration.

## 4. Bivariate statistics

The next step in the exploratory phase of the analysis is to survey
pairs of variables for possible correlation. It is expedient to do this
visually at first, and then to follow up on the relationships that look
promising.

We here have to separate three types of pairs, according to the types of
variables. In our datasets, we will have numeric data (issued from
measurements) and categorical data (issued from classifications).
Accordingly, our pairs can be:

-   numeric-numeric

-   numeric-categorical

-   categorical-categorical

### Pairs of numerical variables

The package `GGally` gives us a good tool to do this quickly for the
numerical variables in the dataset.

```{r}
library(GGally)

shape_data |> 
  select(where(is.numeric)) |> #select only numeric variables 
  drop_na() |> # exclude rows with missing values
  ggpairs() 
```

This plot gives us three elements: \* scatterplots of each variable pair
\* the density curve of each variable that allows us to see the
distribution \* the correlation coefficient (Pearson's r). The strength
of the correlation is marked with asterisks. When there is no asterisk,
there is no significant correlation.

### Numeric - categorical pairs

For pairs of numeric and categorical data, the first step is to create a
series of boxplots.These will allow you to perform visual checks to see
how the data is distributed and whether there might be a relationship.

```{r}
# The function "all_boxplots" will give you a list object
# with a boxplot for each numeric-categorical combination.
# We save it here as an object called 'boxplots'.
boxplots <- all_boxplots(shape_data)

# to see all of them, uncomment and run the following command:
print(boxplots)

# to see any single one, use its number 
boxplots[[21]]
```

Plot 21, for instance, suggests that there is a difference between the
maximum thicknesses of the rims in the different sondages (the
excavation units). Since the units tested different parts of the site,
this is a potentially interesting relationship, and we will follow it up
as an example.

After the visual inspection, we will now attempt to test for the
statistical significance of these relationships using an ANOVA (Analysis
of Variance).ANOVA is a test that needs the data to fulfill two criteria
first: 1. the data needs to be normally distributed 2. variances need to
be equal (don't worry, this will be explained)

Fist, we test for normality. You might have already seen this in the
univariate report and can assume it, but if you want to make
statisticians happy or impress people with numbers, you can do a
normality test (e.g. the Shapiro-Wilks test):

```{r}
# First, run the ANOVA and save its output
res_aov <- aov(mx_th ~ sondage,data = shape_data)

# Then check for normality
shapiro.test(res_aov$residuals)
```

If the p-value is below 0.05, the probability that the data comes from a
normal distribution is very low, and the data is probably not normally
distributed.

If the data is not normally distributed, run a Kruskal-Wallace test. If
it is normally distributed, run ANOVA.

### Kruskal-Wallace test

The Kruskal_Wallace test is run to compare three or more groups in terms
of a quantitative variable. For only two groups, use the Mann-Whitney
test.

```{r}
kruskal.test(mx_th ~ sondage, data = shape_data)
```

Again, we look at the p-value in the result. This tells us that at least
one sondage is significantly different in the thickness of its pottery.
To find out how the groups compare amongst one another, run the Dunnel
test.

```{r}
library(FSA)

dunnTest(mx_th ~ sondage,
  data = shape_data,
  method = "holm"
)
```

In the resulting table, look at the adjusted P-values. This shows us
that the difference between B and T1 is significant (below 0.05), while
the other two pairs are not.

If you want to display all of this in a single graph, use the following:

```{r}
library(ggstatsplot)

ggbetweenstats(
  data = shape_data,
  x = sondage,
  y = mx_th,
  type = "nonparametric", # this selects Kruskal-Wallis over ANOVA
  plot.type = "box",
  pairwise.comparisons = TRUE,
  pairwise.display = "significant",
  centrality.plotting = FALSE,
  bf.message = FALSE
)
```

### ANOVA

If the data is normally distributed, run an ANOVA.

```{r}
# First, run the ANOVA and save its output
res_aov <- aov(mx_th ~ rim_angle,data = shape_data)

# Then check for normality
shapiro.test(res_aov$residuals)
```

The following function gives you all the information you might need, and
much more.

```{r}
ggstatsplot::ggbetweenstats(shape_data, sondage, mx_th)
```

For our use case, we will simply look at the first entries in the header
line. In this case, the p-value is larger than 0.05, which shows us that
the relationship is not statistically significant. In narrative form, we
could write:

> There was no statistically significant relationship between rim
> thickness and excavation unit (Welch's F (2, 2.65) = 7.94, p = 0.08)

The package provides a function which generates a pdf of bivariate
combinations. Look at these and decide which relationships to follow up
on.

```{r}
# Function that creates one of three plots, according to variable type:
#make_quick_plot <- function(data, x_var, y_var, ...){
#  
#  plot_box <- function(df, catvar, numvar, ...){
#  df |> 
#    select({{catvar}}, {{numvar}}) |> 
#    tidyr::drop_na() |> 
#    ggplot(aes(x = reorder({{catvar}}, {{numvar}}), y = {{numvar}})) + 
#    geom_boxplot()+
#    theme_bw()+
#    labs(x = rlang::englue("{{catvar}}"),
#         y = rlang::englue("{{numvar}}"))
#  }
#  
#  plot_bar_group <- function(df, x, y, ...){
#  df |>
#    select({{x}}, {{y}}) |> 
#    tidyr::drop_na() |> 
#    ggplot(aes(x = {{ x }}, fill  = {{ y }}))+
#    geom_bar(position = "dodge")+
#    theme_bw()
#  }
#  
#  
#  if(is.numeric(eval(substitute(x_var), data)) && is.numeric(eval(substitute(y_var), data))){
#  
#    data |> 
#    ggplot(aes(x = {{x_var}}, y = {{y_var}})) + 
#    geom_point()+
#    geom_smooth(method = "lm")+  
#    theme_bw()
#
#  }
#  
#  else if(is.character(eval(substitute(x_var), data)) && is.numeric(eval(substitute(y_var), data))){
#
#    
#    plot_box(data, catvar = {{x_var}}, numvar = {{y_var}})
#    
#  }
#  
#  else if(is.character(eval(substitute(y_var), data)) && is.numeric(eval(substitute(x_var), data))){
#
#    
#    plot_box(data, catvar = {{y_var}}, numvar = {{x_var}})
#    
#  }
#    
#  else if(is.character(eval(substitute(x_var), data)) && is.character(eval(substitute(y_var), data))){
#    if(n_distinct(eval(substitute(x_var), data)) > n_distinct(eval(substitute(y_var), data))){
#      plot_bar_group(data, {{x_var}}, {{y_var}})
#    }
#    else{
#      plot_bar_group(data, {{y_var}}, {{x_var}})
#    }
#  }
#}
#
#
#rims_only |> make_quick_plot(mx_th, rim_angle)
#
#
#
## it might be a good idea to center and standardise numerical values before analysing their distribution. To discuss with Cezary.
#
#
#center_and_standardise <- function(x) (x - mean(x, na.rm = TRUE) / sd(x, na.rm = TRUE))
#
#
#library(ggplot2)
#
#l <- combn(names(rims_only), 2, simplify = FALSE)
#
#l <- purrr::map(l, append, 'rims_only', after = 0)
#
#
#
#p <- do.call(make_quick_plot, as.list(l[[1]]))
#
#purrr::map(l, ~do.call(make_quick_plot, as.list(.x)))
#ldf <- as.data.frame(do.call(rbind, l))
#make_quick_plot(data = eval(parse(text = ldf$V1)), ldf$V2, ldf$V3)
#
#lapply(l, function(x) do.call(make_quick_plot, as.list(x)))
#
#x <- as.list(ldf$V2)
#y <- as.list(ldf$V3)
#
#purrr::map2(x, y, make_quick_plot, data = rims_only)
#
#out <- vector("list", length = length(l))
#for(i in seq_along(l)){
#  
# out[[i]] <-  rims_only |> 
#   ggplot(aes_string(x = l[[i]][1], y = l[[i]][2])) + 
#  geom_point()+
#  geom_smooth(method = "lm")+  
#  theme_bw()
#}
#out[[20]]
#
#
#plot_scatter <- function(data, x_var, y_var){
#  data |> 
#  ggplot(aes(x = .data[[x_var]], y = .data[[y_var]])) + 
#  geom_point()+
#  geom_smooth(method = "lm")+  
#  theme_bw()
#}
#
#
#
#p <- names(rims_only)[-1] |> 
#  purrr::map( ~ make_quick_plot(rims_only, names(rims_only)[1], .x))
#
#p <- l |> 
#  purrr::map( function(x) make_quick_plot(rims_only, l[[x[1]]], l[[x[2]]]))
#
#p[[20]]
#print_2var <- function(data){}
#
#print(p)
#
#
#
## Make a correlation heatmap
#
#library(DataExplorer)
#
#plot_correlation(na.omit(rims_only), maxcat = 5L)
#
#plot_intro(all)
#
#plot_missing(all)
#
#
#
#
#  
```

```{r}
#rethink




# we ought to do the pairwise tests first to decide which relationships are worth looking at
#select numeric variables for further use
nums <- rims_only |> 
  select(where(is.numeric)) |> 
  drop_na()

# make correlation matrix
x <- cor(nums) 
# keep only lower triangle
x[upper.tri(x, diag = TRUE)] <- NA

# put into long form for sorting
cor_numeric <- x |>
  as_tibble(rownames = "var1") |> 
  pivot_longer(cols = -var1, names_to = "var2", values_to = "correlation") |> 
  filter(!correlation == 1) |> 
  arrange(desc(correlation))

# Now for numeric/categorical

# Here, make only the boxplots for visual inspection. Then, explain what to do later if a relationship should look promising.

cats <- rims_only |> 
  select(where(is.character))

x <- crossing(num = names(nums), cat = names(cats))


make_boxplot <- function(numvar, catvar){
  ggplot(rims_only)+
  aes(x = catvar, y = numvar)+
  geom_boxplot()
}
  
make_boxplot(diam, rim_angle)

p <- vector("list", length = nrow(x)) 
  
for (i in 1:nrow(x)) {
  p1 <- 
  make_boxplot(paste0(x$num[i]), paste0(x$cat[i])
  )
  p[[i]] <- p1
  
}
p[[1]]

a <- purrr::map2(x$num, x$cat, make_boxplot)
a[[11]]

#both of these do not make the desired plot

rims_only |> 
  select(diam, mx_th, min_th, rim_angle, sondage) |> 
  ggpairs()

# We need to do an ANOVA test for this kind of thing

#basic anova test:

p <- vector("list", length = nrow(x)) 
  
for (i in 1:nrow(x)) {
  p[[i]] <- 
  oneway.test(x$`names(nums)`[i] ~ x$`names(cats)`[i] ,
  data = rims_only,
  var.equal = TRUE # assuming equal variances
  )
  
}


x$`names(nums)`[1]

nrow(x)



# There is a workflow here: https://statsandr.com/blog/how-to-do-a-t-test-or-anova-for-many-variables-at-once-in-r-and-communicate-the-results-in-a-better-way/

library(ggstatsplot)
library(tibble)

dat <- rims_only

# edit from here
x <- "rim_angle"
cols <- 2:4 # the 4 continuous dependent variables
type <- "parametric" # given the large number of observations, we use the parametric version
paired <- FALSE # FALSE for independent samples, TRUE for paired samples
# edit until here

# edit at your own risk
plotlist <-
  purrr::pmap(
    .l = list(
      data = list(as_tibble(dat)),
      x = x,
      y = as.list(colnames(dat)[cols]),
      plot.type = "box", # for boxplot
      type = type, # parametric or nonparametric
      pairwise.comparisons = TRUE, # to run post-hoc tests if more than 2 groups
      pairwise.display = "significant", # show only significant differences
      bf.message = FALSE, # remove message about Bayes Factor
      centrality.plotting = FALSE # remove central measure
    ),
    .f = ifelse(paired, # automatically use ggwithinstats if paired samples, ggbetweenstats otherwise
      ggstatsplot::ggwithinstats,
      ggstatsplot::ggbetweenstats
    )
  )

# print all plots together with statistical results
for (i in 1:length(plotlist)) {
  print(plotlist[[i]])
}
```

Do correlation tests, then order the plots according to correlation

## Plots

We provide a ggplot theme for fast and unified styling of graphics.

```{r}
ggplot(rims) +
  geom_point(aes(x = diam, y = mx_th, colour = sondage)) +
  theme_potteR()

```

```{r}
ggplot(rims) +
  geom_boxplot(aes(x = sondage, y = mx_th, fill = sondage)) +
  params_potteR

params_potteR <- list(ggplot2::theme_bw(), 
                      khroma::scale_colour_bright())
theme_potteR <- function() {
  ggplot2::theme_bw()+
  khroma::scale_colour_bright()
}
  
```

## Maps

## Seriation

An important aspect of ceramic analysis is to find attributes or types
that seriate. This has two main interpretive ends in archaeology. The
first is to create orders in previously unordered data, for instance in
a series of samples from survey or shovel testing. The second is to
identify developments in series that have pre-defined orders, such as
contexts from stratigraphic excavations.

### Using seriation to order assemblages

### Identifying developments in ordered assemblages

We will here aim our seriation plot at decor types and rim types. We use
the `plot_ford()` function from the
[`tabula`](https://www.tesselle.org/packages.html) package to make these
plots. The function given here adds ease in data preparation.

A predefined order has to be given for the contexts or assemblages. This
is then used as the `context_order` argument in the function.

```{r}
# Stratigraphic order of contexts for Unit B
b_order <- c("1", "2", "3", "4", "5",  "6", "7",  "8",  "9",  "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23",  "24", "25", "26")

rims_only |> 
  filter(sondage == "B") |> # select only Unit B
  plot_series(context, rim_type, b_order)
```

Since the returned plot is a ggplot object, it can be modified using
ggplot syntax to add titles etc., e.g.:

```{r}
rims_only |> 
  filter(sondage == "B") |> # select only Unit B
  plot_series(context, rim_type, b_order) +
  labs(title = "Rim Types from Unit B",
       subtitle = "NDK1")
```

### Temporal development of decors

```{r}
#split all three decor columns off into separate dfs
dec1 <- all_decor_data |> 
  select(sondage, context, dec1) |> 
  rename(dec = dec1)

dec2 <- all_decor_data |> 
  select(sondage, context, dec2) |> 
  rename(dec = dec2)

dec3 <- all_decor_data |> 
  select(sondage, context, dec3) |> 
  rename(dec = dec3)

decors <- bind_rows(dec1,dec2,dec3) |> 
  drop_na()

#seriate for B
decors |> 
  filter(sondage == "B") |> # select only Unit B
  filter(!dec == "ERODE") |> 
  plot_series(context, dec, b_order)+
  labs(title = "Decors from Unit B",
       subtitle = "NDK1, combined body and rim sherds",
       caption = "n=484")
```

## Finding Groups (Juan-Marco)

## Analysing decors and motifs

networks (J-M)

## Significance and Standardisation
